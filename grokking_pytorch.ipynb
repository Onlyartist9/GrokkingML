{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b6f92d",
   "metadata": {},
   "source": [
    "## Experimenting with Pytorch Tensors\n",
    "Tensors are optimized multidimensional arrays. This section of the Notebook is just me experimenting with them to gain a slightly deeper understanding of what they entail. A lot of what is in here are excerpts taken from Deep Learning with Pytorch by Eli Stevens et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple array.\n",
    "a = [1.0,2.0,1.0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdde984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the pytorch library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tensor filled with 3 ones.\n",
    "a = torch.ones(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Element access works just like array access.\n",
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d04f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates an assigns a tensor filled with 6 zeroes.\n",
    "points = torch.zeros(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0] = 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca89f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bf7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[2] = 5.0\n",
    "points[3] = 3.0\n",
    "points[4] = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d7d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([4.0,1.0,5.0,2.0,1.0])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(points[0]),float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d396de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of a multidimensional tensor.\n",
    "torch.tensor([[\n",
    "    [4.,1.],\n",
    "    [5.,3.],\n",
    "    [2.,1.]\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18806a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The dimensions of the tensor.\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd7add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to look into the torch randn function.\n",
    "img_t = torch.randn(3,5,5)\n",
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f06bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([0.2126,0.7152,0.0722])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_t = torch.randn(2,3,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d534a1",
   "metadata": {},
   "source": [
    "## Grokking Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b9fae",
   "metadata": {},
   "source": [
    "### Basics of model fitting. \n",
    "Given two temperature readings. One known(Celsius) the other unknown how do we determine what scale the unknown readings are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,14.0,15.0,28.0,11.0,8.0,3.0,-4.0,6.0,13.0,21.0] #Celsius readings\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4] #Unknown scale.\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c.shape #Shape allows us to determine the dimensions of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03282eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964789a",
   "metadata": {},
   "source": [
    "#### Let's visualize the data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data ={\n",
    "    \"t_c\" : t_c,\n",
    "    \"t_u\" : t_u\n",
    "}\n",
    "dataframe = pd.DataFrame(data)\n",
    "dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=dataframe,x=\"t_c\",y='t_u')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f104c08",
   "metadata": {},
   "source": [
    "## We can see that the two scales are linearly correlated. Meaning that they are different only in some weight(coefficient) and bias. Let's model this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d121482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our model\n",
    "def model(t_u,w,b): #inputs to our models are the weights, biases and the model we're trying fit.\n",
    "    return w*t_u + b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336933e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define our loss function\n",
    "def loss(t_p,t_c):\n",
    "    squared_diffs = (t_p-t_c)**2 #Squared mean error\n",
    "    return squared_diffs.mean() #Takes the mean of all the tensors generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635787e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_c,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ce26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss(t_c,t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc081b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142aced",
   "metadata": {},
   "source": [
    "### Gradient Descent.\n",
    "It is a technique with which we can adjust our parameters to fit our models appropriately(towards a local minimum). The goal is to get the value of the loss function as low as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1 #The value we add or subtract to our parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p,t_c):\n",
    "    squared_diffs = (t_p - t_c)**2 #Mean squared error\n",
    "    return squared_diffs.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_w = (loss_fn(model(t_u,w+delta,b),t_c)) - (loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n",
    "#We calculate the rate of change of loss in the parameters as a way with which we can determine the direction we should descend towards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =  1e-2 #The value with which we adjust our parameters.\n",
    "w = w - learning_rate * loss_rate_of_change_w #Creating a new value for our weights.\n",
    "#Adjusting the model's learning rate is called hyperparameter tuning. We are training/fitting the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25249f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b \n",
    "#Same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p,t_c): # Derivative of our loss function.\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0) #Chain rule on our loss function.\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d49256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u #Derivative of our model with respect to the derivative our model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0 #Derivative of our model with respect to our derivative bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a79a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b): #Gradient descent function.\n",
    "    dloss_dtp = dloss_fn(t_p, t_c) #Derivative of our loss function with respect to derivative of our predicted temp.\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b) #Derivative of our loss wrt derivative of the weights.\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b) #Derivative of our loss wrt to derivative of our bias.\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()]) #It basically assembles a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params #Sets the paremters\n",
    "        t_p = model(t_u, w, b) #assigns the predicted model\n",
    "        loss = loss_fn(t_p, t_c) #Returns the loss of the model(How close it is to the actual model)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b) #Returns the gradient to see the direction with which we should train our model.\n",
    "        params = params - learning_rate * grad # Assigns new parameters based on gradient and learning rate.\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa66439",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "n_epochs = 100,\n",
    "learning_rate = 1e-4,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_u,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u #We normalize the values so they don't go way too out of bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0865319",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = training_loop(\n",
    "n_epochs = 5000,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0]),\n",
    "t_u = t_un,\n",
    "t_c = t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the model again\n",
    "sns.relplot(x=t_p, y=t_c) #We see that we were closely able to fit our unknown model.\n",
    "sns.relplot(x=t_u,y=t_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d962c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262da47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5ad0d",
   "metadata": {},
   "source": [
    "## Autograd. \n",
    "A technique that automatically calculates the gradient of the parameters at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f21793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u,w,b):\n",
    "    return w*t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4dc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e851349",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0],requires_grad = True) #sets autogradient. Keeps/Accumulates gradients of tensors whenever calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.grad is None #As of now we can see that the gradients for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc47fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(t_u,*params),t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward() #We use backprop to calculate the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training epochs in loop\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1): #Loop based on number of epochs.\n",
    "        \n",
    "        if params.grad is not None: #If we have calculated the gradient at some point in time we set the gradient back to zero.\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params) #Set the tensors of the predicted temperatures\n",
    "        loss = loss_fn(t_p, t_c) # Calculate the loss\n",
    "        loss.backward() #Calculate the gradient of the loss.\n",
    "        \n",
    "        with torch.no_grad(): #Need to understand torch.no_grad\n",
    "            params -= learning_rate * params.grad  #Adjust params\n",
    "    if epoch % 500 == 0:\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7268cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "n_epochs = 5000,\n",
    "learning_rate = 1e-2,\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1710482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim #Optimizer library. We can use it to access a bunch of gradient descent optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(optim) # A list of optimizers in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8739bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0,0.0],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a40ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b022c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([params], lr=learning_rate) #Here we make use of SGD(Stochastic Gradient Descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p = model(t_u,*params)\n",
    "loss = loss_fn(t_p,t_c)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeed9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with optimizer like SGD\n",
    "def training_loop(n_epochs,optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1): #Loop based on number of epochs.\n",
    "        t_p = model(t_u, *params) #Set the tensors of the predicted temperatures\n",
    "        loss = loss_fn(t_p, t_c) # Calculate the loss\n",
    "        loss.backward() #Calculate the gradient of the loss.\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c52822",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loop(\n",
    "n_epochs = 5000,\n",
    "optimizer = optimizer,\n",
    "params = params, \n",
    "t_u = t_un,\n",
    "t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8fb21",
   "metadata": {},
   "source": [
    "# The essence of deep learning\n",
    "\n",
    "The essence of Deep learning is model fitting. We look at our data, look at discrepancies, select a model with various parameters, select an error metric/cost/loss function, then perform gradient descent multiple times over.\n",
    "\n",
    "In pytorch the steps approximate to:\n",
    "1. Explore the data\n",
    "2. Select a model/architecture with its parameters.\n",
    "3. Create a loss function\n",
    "4. Perform gradient descent with backprop multiple times over(The training loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc218760",
   "metadata": {},
   "source": [
    "## Creating train/test and validation sets\n",
    "To see how well our model is actually learning we split our datasets into two parts, validation and test set. The validation set has the sole purpose of seeing how well our model performs on data it isn't been trained on. If there's an increasing divergence between the validation and training sets, the model is overfitting. If both are stagnant, the model isn't learning. They need to steadily converge to ensure we are making progress and that the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = t_u.shape[0] #We check the number of values in our unknown scale.\n",
    "n_val = int(0.2 * n_samples) # We take 20% of the values for use as our validation set.\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples) #We create a tensor of 11 indexed values.\n",
    "train_indices = shuffled_indices[:-n_val] #Use some of those indexes for our training set\n",
    "val_indices = shuffled_indices[-n_val:] # Use some of those indexes for our validation set\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "\n",
    "train_t_un = 0.1 * train_t_u #Normalize the training set for the unknown scale\n",
    "val_t_un = 0.1 * val_t_u#Normalize the validation set for the unknown scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4d098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "    train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params) #We model both the training and validation sets, applying a loss to both.\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        val_t_p = model(val_t_u, *params) \n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "    \n",
    "training_loop(\n",
    "n_epochs = 3000,\n",
    "optimizer = optimizer,\n",
    "params = params,\n",
    "train_t_u = train_t_un,\n",
    "val_t_u = val_t_un, \n",
    "train_t_c = train_t_c,\n",
    "val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fdb7a",
   "metadata": {},
   "source": [
    "## Assuming the model was w2 * t_u ** 2 + w1 * t_u + b instead? How would we adapt our program to fit this new definition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newmodel(t_u,w2,w1,b):\n",
    "    return w2 * t_u ** 2 + w1 * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b45b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "    train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = newmodel(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        val_t_p = newmodel(val_t_u, *params) \n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "            f\" Validation loss {val_loss.item():.4f}\")\n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba7bd72be999ae1d378afc5aa895eaeb09b484a8a4ffe8d826cdbd90f9da4008"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
